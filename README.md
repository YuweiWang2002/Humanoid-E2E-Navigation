# 人形机器人端到端循迹与避障项目

本项目旨在构建一个端到端的深度学习模型，使人形机器人在高人流密度的复杂场景（如机器人大会会场）中，能够实现稳健、平滑的自主循迹与动态避障功能。

## 核心任务

通过学习人类专家的驾驶数据，模型直接从传感器输入映射到机器人的运动控制指令，实现“所见即所行”的端到端导航。

### 模型输入

模型采用多模态输入，以全面感知周围环境和自身状态：

1.  **深度图像 (必需)**:
    *   来源: 人形机器人头部 Realsense 相机。
    *   规格: 640x480, 单通道。
    *   作用: 提供场景的三维空间信息，用于检测障碍物和可通行区域。
2.  **定位与朝向数据 (必需)**:
    *   来源: UWB 定位系统（提供二维坐标 `x, y`）和机器人自身IMU（提供航向角 `yaw`）【这个地方我有点忘了yaw是怎么计算出来的，到底是绝对的朝向(在UWB提供的世界坐标系中)还是机器人自身坐标系下的。】。
    *   处理: 在数据处理阶段，这些原始数据被转换为相对于下一个路径点的 **目标距离** 和 **目标方向角**。这两个值作为模型的直接输入，引导机器人朝目标前进。
3.  **RGB 视觉图像 (可选)**:
    *   来源: 人形机器人头部 Realsense 相机。
    *   规格: 640x480, 三通道。
    *   作用: 可以选择性地加入训练，为模型提供更丰富的环境纹理信息。项目初期为了降低对视觉的依赖和模型的复杂性，主要依赖深度图。

### 模型输出

模型直接输出机器人底盘的实时速度指令：

*   **线速度 (vel_x, vel_y)**: 机器人在其自身坐标系下的前向和侧向速度。
*   **角速度 (vel_yaw)**: 机器人的旋转速度。


## **Getting Started**
### Requirements
- python 3.8 
- PyTorch
- torchvision
- matplotlib==3.4.2
- numpy==1.21.2
- pandas==1.3.0
- seaborn==0.11.1
- pillow==8.3.1
- opencv-python
- wandb

### 环境安装
```bash
pip install -r requirements.txt
```


## **网络结构**

本项目的核心网络结构是一个多模态的 **CNN-RNN** 混合模型，旨在同时处理视觉空间信息和时间序列数据。

1.  **CNN (卷积神经网络)**:
    *   负责处理输入的 640x480 深度图。
    *   网络内部包含下采样层，以适应高分辨率输入，有效提取环境的空间特征。
    *   输出128维。
2.  **MLP (多层感知机)**:
    *   负责处理 **[目标距离, 目标方向角]** 这二维的引导数据。
    *   将该低维数据映射到一个高维特征空间（例如64维），使其能与视觉特征有效融合。
    *   输出64维。
3.  **特征融合**:
    *   将 CNN 提取的视觉特征与 MLP 提取的引导特征进行拼接（Concatenate）。
    *   共128+64=192维。
4.  **RNN (循环神经网络, 如 LSTM/GRU)**:
    *   接收融合后的特征向量序列。
    *   通过其内部的记忆单元，捕捉机器人运动和环境变化的**时间依赖性**，使得决策更加平滑和具有预见性。
5.  **输出层**:
    *   一个全连接层，将 RNN 的输出解码为最终的 3 个速度控制指令 (`velx`, `vely`, `vel_yaw`)。

## **模型参数与维度**

为了更好地理解模型的构成和复杂度，下表详细列出了不同模块的输入输出维度以及不同模型组合的参数量。

### 数据流维度

**场景一：仅使用深度图 (Depth + State)**

| 阶段 | 数据来源 | 输入维度 | 输出维度 | 备注 |
| :--- | :--- | :--- | :--- | :--- |
| CNN 特征头 | 深度图像 | `(B, T, 1, 480, 640)` | 128 | `B`:批大小, `T`:时间序列长度 |
| MLP 特征头 | 状态数据 | `(B, T, 2)` | 64 | `[距离, 角度]` |
| 特征融合 | CNN + MLP | - | 192 | `128 + 64` |
| RNN 模型 | 融合后特征 | 192 | 隐藏层维度 | - |
| 输出层 | RNN 输出 | 隐藏层维度 | 3 | `[velx, vely, vel_yaw]` |

**场景二：使用深度图与RGB图 (Depth + State + RGB)**

| 阶段 | 数据来源 | 输入维度 | 输出维度 | 备注 |
| :--- | :--- | :--- | :--- | :--- |
| CNN 特征头 (Depth) | 深度图像 | `(B, T, 1, 480, 640)` | 128 | - |
| CNN 特征头 (RGB) | RGB 图像 | `(B, T, 3, 480, 640)` | 128 | - |
| MLP 特征头 | 状态数据 | `(B, T, 2)` | 64 | `[距离, 角度]` |
| 特征融合 | CNNs + MLP | - | 320 | `128 + 128 + 64` |
| RNN 模型 | 融合后特征 | 320 | 隐藏层维度 | - |
| 输出层 | RNN 输出 | 隐藏层维度 | 3 | `[velx, vely, vel_yaw]` |


### 模型参数量对比

**场景一：仅使用深度图 (Depth + State)**

| RNN 模型 | Nvidia Head | ResNet Head | AlexNet Head |
| :--- | :---: | :---: | :---: |
| **LSTM** | 168,999 | 516,899 | 223,063 |
| **GRU** | 152,487 | 500,387 | 206,551 |
| **CT-GRU** | 382,567 | 730,467 | 436,631 |

**场景二：使用深度图与RGB图 (Depth + State + RGB)**

| RNN 模型 | Nvidia Head | ResNet Head | AlexNet Head |
| :--- | :---: | :---: | :---: |
| **LSTM** | 299,355 | 995,155 | 407,483 |
| **GRU** | 274,651 | 970,451 | 382,779 |
| **CT-GRU** | 619,419 | 1,315,219 | 727,547 |

## **数据与处理**

### 数据采集
数据通过 ROS 在真实机器人上采集。由人类专家遥控机器人，同时记录下传感器数据和对应的控制指令。

### 数据格式
采集的每一条轨迹都存储在一个 `.csv` 文件中，例如 `data/processed/trajectory_1.csv`。文件格式如下：

```csv
depth_filename,rgb_filename,distance_to_target,angle_to_target,vel_x,vel_y,vel_yaw
depth/1752480911478.png,rgb/1752480911478.png,2.870,-0.317,0.604,0.0,-0.150
...
```
*   `depth_filename`: 深度图像文件路径。
*   `rgb_filename`: RGB图像文件路径。
*   `distance_to_target`: 当前位置到下一个目标点的距离（米）。
*   `angle_to_target`: 机器人当前朝向与目标点方向之间的夹角（弧度）。
*   `vel_x`, `vel_y`, `vel_yaw`: 专家在该时刻输入的控制指令，作为训练标签（Ground Truth）。


## **模型训练**

推荐使用 `train.sh` 脚本来启动模型训练。该脚本封装了 `train_all_models.py` 的常用参数，方便您快速开始。

```bash
bash train.sh
```


## **模型测试与部署**

训练好的模型可以部署到机器人上进行在线测试。
1.  编写一个 `ros_control.py` 的ROS节点。
2.  该节点加载训练好的PyTorch模型。
3.  订阅机器人发布的传感器话题（深度图、UWB、IMU等）。
4.  在回调函数中进行数据预处理，将数据送入模型进行推理。
5.  将模型输出的速度指令发布到机器人的 `cmd_vel` 话题，控制机器人运动。
