# 人形机器人端到端导航项目

本项目旨在构建一个端到端的深度学习模型，使人形机器人在复杂、人流密集的真实场景中，能够实现稳健、平滑的自主导航与动态避障。

## 核心任务

通过学习人类专家的驾驶数据（行为克隆），模型直接从传感器输入映射到机器人的运动控制指令，实现“所见即所行”的端到端导航模式。

### 模型输入

模型采用多模态输入，以全面感知周围环境和自身状态：

1.  **深度图像 (必需)**:
    *   **来源**: 人形机器人头部的 Realsense 相机。
    *   **规格**: 640x480, 单通道。
    *   **作用**: 提供场景的三维空间信息，用于检测障碍物和可通行区域。

2.  **状态向量 (必需, 5维)**:
    *   **来源**: 结合了用于定位目标的系统（如UWB）和机器人自身的里程计（Odometry）信息。
    *   **构成**: 状态向量为 `[目标距离, 目标方向角, 当前线速度x, 当前线速度y, 当前角速度yaw]`。
    *   **作用**: 这是至关重要的输入。它不仅告诉模型**要去哪里**（距离和角度），更提供了关于机器人**当前动量**的关键上下文信息。这使得RNN能够学习动力学，从而生成更平滑、更符合物理规律的控制指令。

3.  **RGB 视觉图像 (可选)**:
    *   **来源**: 人形机器人头部的 Realsense 相机。
    *   **规格**: 640x480, 三通道。
    *   **作用**: 可以选择性地加入训练，为模型提供更丰富的环境纹理和颜色信息。

### 模型输出

模型直接输出机器人底盘的实时速度指令：

*   **线速度 (`cmd_vel_x`, `cmd_vel_y`)**: 机器人在其自身坐标系下的前向和侧向速度。
*   **角速度 (`cmd_vel_yaw`)**: 机器人的旋转速度。

---

## 快速开始

### 环境要求

请确保您已安装 Python 3.8+ 及相关依赖包。

```bash
# 强烈建议使用虚拟环境
python -m venv venv
source venv/bin/activate  # 在 Windows 上请使用 `venv\Scripts\activate`

# 安装依赖
pip install -r requirements.txt
```

### 数据生成与预处理

本项目包含用于测试完整流程的虚拟数据生成脚本。

1.  **生成原始数据**: 创建虚拟的轨迹、图像，并计算所有状态和标签信息。
    ```bash
    python generate_dummy_data.py
    ```

2.  **预处理数据**: 选择必要的列，并将数据保存到已处理文件夹中。
    ```bash
    python preprocess_data.py
    ```

### 数据归一化 (重要)

本项目采用在线/增量标准化的方式来处理数据，特别是深度图。这允许我们在数据不断增加（例如，从仿真到真实世界）的情况下，动态更新全局的均值和标准差，以保证数据分布的稳定性。

- **统计文件**: 归一化参数（均值、标准差）被保存在根目录的 `depth_normalization.json` 文件中。
- **更新统计数据**: 当你添加了大量新的、不同分布的数据后，你应该更新这些统计数据。使用 `--update_stats` 标志来运行训练脚本，此时脚本会先遍历所有训练数据来计算新的统计量，然后才开始训练。

    ```bash
    # 推荐在第一次训练或添加新场景数据后运行此命令
    python train_all_models.py --config configs/base_config.yml --update_stats
    ```

---

## 如何训练

训练流程由一个中心的 YAML 配置文件管理。所有实验参数，从模型架构到学习率，都在此定义。

1.  **配置你的实验**:
    *   打开 `configs/base_config.yml` 文件。
    *   根据需要修改参数。例如，更改 `run_name`，选择 `rnn_type` (`GRU`, `LSTM`, `CTGRU`)，或调整 `learning_rate`。
    *   若要进行一个全新的实验，你可以复制 `base_config.yml` 并重命名（例如 `configs/lstm_experiment.yml`），然后修改它。

2.  **开始训练**:
    *   运行训练脚本，并指定你想要使用的配置文件。如果不是第一次训练，则无需带 `--update_stats` 标志。

    ```bash
    python train_all_models.py --config configs/base_config.yml
    ```

    *   若要运行不同的实验，只需更改配置文件的路径：

    ```bash
    python train_all_models.py --config configs/lstm_experiment.yml
    ```

--- 

## 网络结构

核心网络是一个多模态的 **CNN-RNN** 混合模型，旨在同时处理视觉空间信息和时间序列数据。

1.  **CNN (卷积神经网络)**: 处理输入的深度图（以及可选的RGB图），提取空间特征。
2.  **MLP (多层感知机)**: 处理5维的状态向量 `[距离, 角度, 线速度x, 线速度y, 角速度yaw]`，并将其映射到更高维的特征空间。
3.  **特征融合**: 将 CNN 提取的视觉特征与 MLP 提取的状态特征进行拼接（Concatenate）。
4.  **RNN (循环神经网络, 如 LSTM/GRU)**: 接收融合后的特征向量序列，通过其内部的记忆单元捕捉时间依赖性，使得决策更加平滑和具有预见性。
5.  **输出层**: 一个全连接层，将 RNN 的输出解码为最终的3个速度控制指令。

### 模型参数与维度

*注意: 以下参数量是基于旧的2维状态输入估算的。新的5维状态输入仅对MLP头的参数量有极小的增加，不会显著改变模型总体的参数规模。* 

**场景一: 仅使用深度图 (Depth + 5D State)**

| RNN 模型 | Nvidia Head | ResNet Head | AlexNet Head |
| :--- | :---: | :---: | :---: |
| **LSTM** | ~16.9万 | ~51.7万 | ~22.3万 |
| **GRU** | ~15.2万 | ~50.0万 | ~20.7万 |
| **CT-GRU** | ~38.3万 | ~73.0万 | ~43.7万 |

**场景二: 使用深度图与RGB图 (Depth + RGB + 5D State)**

| RNN 模型 | Nvidia Head | ResNet Head | AlexNet Head |
| :--- | :---: | :---: | :---: |
| **LSTM** | ~29.9万 | ~99.5万 | ~40.7万 |
| **GRU** | ~27.5万 | ~97.0万 | ~38.3万 |
| **CT-GRU** | ~61.9万 | ~131.5万 | ~72.8万 |

---

## 机器人部署

模型训练完成后，可以将其部署到真实机器人上进行在线推理。

1.  编写一个用于控制的 ROS 节点。
2.  该节点加载训练好的 PyTorch 模型（`.pth` 文件）。
3.  订阅必要的传感器话题：`/camera/depth/image_raw`（深度图）、`/odom`（里程计信息）以及提供目标点的相关话题。
4.  在回调函数中，对传感器数据进行预处理，使其符合模型的输入格式（5维状态向量 + 图像张量）。
5.  运行模型进行推理。
6.  将模型输出的速度指令发布到机器人的 `/cmd_vel` 话题上。
